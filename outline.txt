Overview
BigGlue specifies its pipes as a relationship or wiring between computations and data stores. There are 3 different types of computations in BigGlue: A simple mapping step, which converts an input into a list of outputs, would be specified with a mapping computation Mapping, and would be specified in the pipeline as store1:--Mapping-->store2. A Pairwise Computation step has a filter and a way to converge two streams into a single computation step. For example, a Cartesian Product step would simply pair two sets of inputs together and store them into an output step specified like (input1 || input2) :-*CartesianProduct*-> output. A Reduction step would then allow you to accumulate data onto a field starting with an initial condition, acting quite a bit like the reduce or fold steps in MapReduce or Scala, which can be specified as input :-+Reduce+->output.

Pipes also have data connectors throughout its steps, which can be considered as “Platforms”, which allow you to specify how to send your data into the different steps of the pipeline. The default for this is a connector that passes data through one to one, but with a status map which updates from “Not Done” to “Done” once the platform has finished computing that data. This connector also has a filter, blocking data that it had marked as “Done” to prevent redundant computation. However, connectors can be changed per pipeline or per step to just be one to one, or to send behavior in time and/or data intervals, or some of each by sequencing connectors together.

This of course, forces you to store intermediate data into the pipelines somewhere, but they can simply be in memory if you don’t really care about the results. If you do care about the intermediate data, and have prototyped your pipeline in memory, you can also change DataMaps easily with only a bit of serialization changes, which allows you to have your intermediate steps available to look at with minimal effort. In this case, the mappingSteps are also connected to the stores. This allows you to easily model different imperative steps or cycles by having something like store1:--mappingStep-->(store2, store1), if store1 got changed in any way. If any item got added to store1 in this case, it will then go back to the beginning of the pipeline.

These pipelines, rather than just taking in specific data types, usually take in Identifiables, which are wrappers to objects that also have an Identity mapped to it. This makes it easy to put things into DataMaps. Identifiables also allow us to add data to these wrappers depending on our needs, such as versions, provenance tracking via different curators, or whether or not an Identifiable has been processed before if you’re putting that Identifiable in a Status Map.

BigGlue also allows you to list different steps and different Identifiables as having different versions via a version curator. As such, you can modify different steps and change the version for those steps. (TODO: Have a snapshot-like version like with SBT?) This in BigGlue should then allow you to recompute certain parts of the pipeline that have already been marked as computed, creating different Identifiables with different version numbers. From there, they should flow down the pipeline. This should eventually allow you to change a step without recomputing unnecessary steps at the very beginning, and will at least allow you to recompute certain steps if you’ve changed a step without wiping intermediate data you might already have.



-----To Be Discussed/Need To Implement------
Since BigGlue keeps its data from intermediate steps, we can increase fault tolerance to the sudden failure of the pipeline. If the pipeline is closed for some reason, we can use information from the intermediate steps to continue on with the pipeline. Platforms track provenance and some connectors can update a status map. Assuming provenance tracking is turned on, when we start up the pipeline we can go up the pipeline, having the platforms check which items in the input map have led to the items in the output maps, and send the remaining inputs down the pipeline through that step. When checking the inputs, the connector can choose which items to send down. As an example, one of the connectors also has a possible Status map that it can use, so we can use that information from the connector to send down data that was marked as “Not Done” or “Modified”.
--------------------------------------------

Case Study: Git Repos
In the very beginning of the Fixr Pipeline, we clone and Github repos in order to get various versions of the repos to then analyze. However, Git repos are imperative, as new commits can be pushed to the repo at any point. In this sense, we will have to continually check the Git Repo to see if there have been any changes. We can model this behavior simply with something like toClone:--Clone()-->cloned:--GetCommits()-->(commits, toPull:--Pull()-->cloned). toPull:--Pull()-->cloned can then be configured so it is called every week to not use up a ton of computation power.

Case Study: Groums
Groums are a pretty weird case in our Fixr Pipeline. Within this part of the pipeline, groums are created via a pair between an APK and a github repo. Groums then need to be accumulated in a group based on where they came from, which can be done by accumulating groums via part of the provenance information, which we have due to the provenance curators. From there, groums need to be analyzed to form a frequent itemset within those groups, leaving us with a cluster of groums that we need to analyze. In this case, we can use the provenance information embedded into the Identifiables of the groums in order to group them together, 

Case Study: Sorting Batches
This is a really silly toy example, but it seems relevant anyway. Let’s say that a computational step runs slightly faster if the data is brought into the step in alphabetical order and must be run one bit at a time (like in a reduction step). In this case, we would use an extension to the Accumulator connector, which will accumulate an x amount of data before signaling that it has new data. However, when signaling down, we are then able to sort the data alphabetically by identity, which has the platform extract the data alphabetically. Again, this is a pretty silly example, but it shouldn’t be hard to imagine cases where order does matter throughout the computation, whether it is speed or if it’s a step that is non-commutative and has to be done with data with a specific order.

Case Study: Expensive Computation Evasion --Part of TBD--
Our pipeline may be on OpenStack or AWS in the future, or parts of it could be on those locations. However, the OpenStack or AWS instance that we could use could potentially go down in the middle of our computations. Alternatively, let’s say that a step near the end of the pipeline was proven to not be correct, and thus the pipeline would have to be shut down to fix that specific step. With a normal pipeline, both of these circumstances would be devastating, as you would potentially have to recompute items over and over again, and you might even have to throw away (or at least move) some of your intermediate data. Moreover, if there were an expensive computation halfway through the pipeline, you would be wasting resources recomputing that section over again. With versioning and the idea of persistence of BigGlue, if the pipeline fails for whatever reason, it could just start back up as though nothing had happened, and an outage wouldn’t waste a single bit of computing power.

Case Study: Verification of Results
In order to actually prove that our pipeline is actually accurate, the developers of a pipeline would want to actually look at the results of a computation and see if those results are as expected. As such, when looking at the database or the datamap of the stored data, if a developer has provenance tracking turned on, they can see the identity of the original data. They can also potentially check it through a modified Provenance curator; When applying a provenance to an Identifiable, it can then look at that Identifiable in the provenance map to see if it already has a provenance. If so, then the developer would be able to compare the provenances in order to at least verify that the results are stable.

