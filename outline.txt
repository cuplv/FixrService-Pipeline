Overview
BigGlue specifies its pipes as a relationship or wiring between computations and data stores. There are 3 different types of computations in BigGlue: A simple mapping step, which converts an input into a list of outputs, would be specified with a mapping computation Mapping, and would be specified in the pipeline as store1:--Mapping-->store2. A Pairwise Computation step has a filter and a way to converge two streams into a single computation step. For example, a Cartesian Product step would simply pair two sets of inputs together and store them into an output step specified like (input1 || input2) :-*CartesianProduct*-> output. A Reduction step would then allow you to accumulate data onto a field starting with an initial condition, acting quite a bit like the reduce or fold steps in MapReduce or Scala, which can be specified as input :-+Reduce+->output.

Pipes also have data connectors throughout its steps, which can be considered as “Platforms”, which allow you to specify how to send your data into the different steps of the pipeline. The default for this is a connector that passes data through one to one, but with a status map which updates from “Not Done” to “Done” once the platform has finished computing that data. This connector also has a filter, blocking data that it had marked as “Done” to prevent redundant computation. However, connectors can be changed per pipeline or per step to just be one to one, or to send behavior in time and/or data intervals, or some of each by sequencing connectors together.

This of course, forces you to store intermediate data into the pipelines somewhere, but they can simply be in memory if you don’t really care about the results. If you do care about the intermediate data, and have prototyped your pipeline in memory, you can also change DataMaps easily with only a bit of serialization changes, which allows you to have your intermediate steps available to look at with minimal effort. In this case, the mappingSteps are also connected to the stores. This allows you to easily model different imperative steps or cycles by having something like store1:--mappingStep-->(store2, store1), if store1 got changed in any way. If any item got added to store1 in this case, it will then go back to the beginning of the pipeline.

These pipelines, rather than just taking in specific data types, usually take in Identifiables, which are wrappers to objects that also have an Identity mapped to it. This makes it easy to put things into DataMaps. Identifiables also allow us to add data to these wrappers depending on our needs, such as versions, provenance tracking via different curators, or whether or not an Identifiable has been processed before if you’re putting that Identifiable in a Status Map.

BigGlue also allows you to list different steps and different Identifiables as having different versions via a version curator. As such, you can modify different steps and change the version for those steps. (TODO: Have a snapshot-like version like with SBT?) This in BigGlue should then allow you to recompute certain parts of the pipeline that have already been marked as computed, creating different Identifiables with different version numbers. From there, they should flow down the pipeline. This should eventually allow you to change a step without recomputing unnecessary steps at the very beginning, and will at least allow you to recompute certain steps if you’ve changed a step without wiping intermediate data you might already have.



Since BigGlue keeps its data from intermediate steps, we can increase fault tolerance to the sudden failure of the pipeline. If the pipeline is closed for some reason, we can use information from the intermediate steps to continue on with the pipeline. Platforms track provenance and some connectors can update a status map. When checking the inputs, the connector can choose which items to send down. As an example, one of the connectors also has a possible Status map that it can use, so we can use that information from the connector to send down data that was marked as “Not Done” or “Modified”.

Possible Idea?: (Assuming provenance tracking is turned on, when we start up the pipeline we can go up the pipeline, having the platforms check which items in the input map have led to the items in the output maps, and send the remaining inputs down the pipeline through that step.)


Case Study: Imperative Steps
In some pipelines, we there could potentially be steps where the data is looked over again and again, or where the data that we look at may change as time goes on. From this, we can model the behavior of checking the data over and over again either by modeling that imperative data as a Cyclic pipeline. From there, we can either pass reactive data through that pipeline, or allow repeated data to go through, being more imperative, depending on the connector that's being used.

At the very start of the Fixr Pipeline, we need to clone and check out commits of Github repos in order to analyze those repos. However, Git Repos themselves are imperative, as new commits can be pushed into the repo at any point. In this sense, we would need to keep checking the Git Repos to see if they are being pushed. We can model this behavior with a pipeline similar to toClone:--Clone()-->cloned:--GetCommits()-->(commits, toPull:--Pull()-->cloned. This pipeline will push clone Github Repos, and then when getting new commits, will also push the repo into the toPull pipeline, which will then allow it to be pulled and sent back to the cloned:--GetCommits()-->commits step. However, while the other steps can be simple streaming steps, doing the same to this step will lead to an infinite computation loop doing absolutely nothing most of the time, wasting computation power. To prevent this, we can change the connector of this step to wait a week before it actually pulls the repos through the configuration file. From there, we would either need to change the commit extraction step to not have a status map, as a status map would most likely block the repos coming back from the Pull step from running if there were no new commits, or take a more reactive approach and add a date state to the repos to allow the pipeline to treat it as different data.

Case Study: Groums
Groums are a pretty weird case in our Fixr Pipeline. Within this part of the pipeline, groums are created via a pair between an APK and a github repo. Groums then need to be accumulated in a group based on where they came from, which can be done by accumulating groums via part of the provenance information, which we have due to the provenance curators. From there, groums need to be analyzed to form a frequent itemset within those groups, leaving us with a cluster of groums that we need to analyze. In this case, we can use the provenance information embedded into the Identifiables of the groums in order to group them together, and store in the File System as a cluster. From there, we can use mapping to form the frequent item set within the cluster of groums. 

Case Study: Expensive Computation Evasion --Kind of Part of TBD?--
Our pipeline may be on OpenStack or AWS in the future, or parts of it could be on those locations. However, the OpenStack or AWS instance that we could use could potentially go down in the middle of our computations. Alternatively, let’s say that a step near the end of the pipeline was proven to not be correct, and thus the pipeline would have to be shut down to fix that specific step. With a normal pipeline, both of these circumstances would be devastating, as you would potentially have to recompute items over and over again, and you might even have to throw away (or at least move) some of your intermediate data. Moreover, if there were an expensive computation halfway through the pipeline, you would be wasting resources recomputing that section over again. With versioning and the idea of persistence of BigGlue, if the pipeline fails for whatever reason, it could just start back up as though nothing had happened, and an outage wouldn’t waste a single bit of computing power.
Let's take the Fixr Pipeline again as an example. Throughout the beginning of the pipeline, we will need to extract commits from Github Repos to then use for our other steps. However, changing these other steps may require extracting commits from the repos again, which is unnecessary. Using the status maps from one of the connectors, we can cut out the 

Case Study: Verification of Results
In order to actually prove that our pipeline is actually accurate, the developers of a pipeline would want to actually look at the results of a computation and see if those results are as expected. As such, when looking at the database or the datamap of the stored data, if a developer has provenance tracking turned on, they can see the identity of the original data. They can also potentially check it through a modified Provenance curator; When applying a provenance to an Identifiable, it can then look at that Identifiable in the provenance map to see if it already has a provenance. If so, then the developer would be able to compare the provenances in order to at least verify that the results are stable.

