What is the problem? Ideas of pipelines currently, or relationships between data and computations, are very useful in the way that we compute data. However, these have their limitations, like having to specify to store data over and over again, or losing data if a pipeline is abruptly shut down or needs to be changed. Moreover, pipelines can change over time, or can be imperative, making them unpredictable These are impossible to model with other libraries like Summingbird, which allows pipelines to be real-time or batch with implementations of Storm, which is a real-time computation platform for pipelines or Scalding, which is a batch computation platform for pipelines, or the libraries that it uses, which expects acyclic graphs.

Why is this an important problem? Specifying something like data stores for intermediate values multiple times in pipelines requires a lot of extra work, and for complex problems can have the developer to go out of their way to implement these issues. E.g. Fixr stopped using Spark because they had to go out of their way to implement some sort of incremental processing for every single step in their pipeline, leading to immense tedium, and a ton of changes if the project guidelines ever changed. Requirements for the pipeline can also change at any point from a prototyping phase to supporting a full project, with not a ton of support for adjusting from memory to a new data system. Moreover, if someone makes a mistake in the pipeline and has to re-run the entire pipeline, you could potentially lose a ton of computation work, recomputing things that have already been computed before.

Why is it hard? There are infinitely many exceptions to the rule. Some steps require different needs; Some steps may require to be run every week, while some are required to run almost instantly. Moreover, there will certain processes and certain requirements that you can't really think of, like some rare variation of provenance tracking or incremental processing. In order to use those, or any unique pattern, you most likely will require extra steps or extensions to all the steps that you have written, leaving the problem only partially solved, if they can be solved at all. 


Overview
BigGlue specifies its pipes as a relationship or wiring between computations and data stores. There are 3 different types of computations in BigGlue: A simple mapping step, which converts an input into a list of outputs, would be specified with a mapping computation Mapping, and would be specified in the pipeline as store1:--Mapping-->store2. A Pairwise Computation step has a filter and a way to converge two streams into a single computation step. For example, a Cartesian Product step would simply pair two sets of inputs together and store them into an output step specified like (input1 | input2) :-*CartesianProduct*-> output. A Reduction step would then allow you to accumulate data onto a field starting with an initial condition, acting quite a bit like the reduce or fold steps in MapReduce or Scala, which can be specified as input :-+Reduce+->output.

Pipes also have data connectors throughout its steps, which can be considered as “Platforms”, which allow you to specify how to send your data into the different steps of the pipeline. The default for this is a connector that passes data through one to one, but with a status map which updates from “Not Done” to “Done” once the platform has finished computing that data. This connector also has a filter, blocking data that it had marked as “Done” to prevent redundant computation. However, connectors can be changed per pipeline or per step to just be one to one, or to send behavior in time and/or data intervals, or some of each by sequencing connectors together. 
Of course, not all steps need a status map, and likewise some steps need some sort of batch processing, or need to wait a little bit before sending data. Likewise, we allow connectors to change step by step to provide the needs of how data will be sent in and out. 

This of course, forces you to store intermediate data into the pipelines somewhere, but they can simply be in memory if you don’t really care about the results. If you do care about the intermediate data, and have prototyped your pipeline in memory, you can also change DataMaps easily with only the need of a serializer, which allows you to have your intermediate steps available to look at with minimal effort. In this case, the mappingSteps are also connected to the stores. This allows you to easily model different imperative steps or cycles by having something like store1:--mappingStep-->(store2, store1), if store1 got changed in any way. If any item got added to store1 in this case, it will then go back to the beginning of the pipeline.

These pipelines, rather than just taking in specific data types, usually take in Identifiables, which are wrappers to objects that also have an Identity mapped to it. This makes it easy to put things into DataMaps. Identifiables also allow us to add data to these wrappers depending on our needs, such as versions, provenance tracking via different curators, or whether or not an Identifiable has been processed before if you’re putting that Identifiable in a Status Map.

BigGlue also allows you to list different steps and different Identifiables as having different versions via a version curator. As such, you can modify different steps and change the version for those steps. (TODO: Have a snapshot-like version like with SBT?) This in BigGlue should then allow you to recompute certain parts of the pipeline that have already been marked as computed, creating different Identifiables with different version numbers. From there, they should flow down the pipeline. This should eventually allow you to change a step without recomputing unnecessary steps at the very beginning using the ideas from the paragraphs below, and will at least allow you to recompute certain steps if you’ve changed a step with or without wiping intermediate data you might already have.

Since BigGlue keeps its data from intermediate steps, we can increase fault tolerance to the sudden failure of the pipeline. If the pipeline is closed for some reason, we can use information from the intermediate steps to continue on with the pipeline. Platforms track provenance and some connectors can update a status map. When checking the inputs, the connector can choose which items to send down. As an example, one of the connectors also has a possible Status map that it can use, so we can use that information from the connector to send down data that was marked as “Not Done” or “Modified”.

(NOTE: This idea above is extremely similar to one of the ones introduced in the Köhler's paper Improving Workflow Fault Tolerance through Provenance-Based Recovery. However, we can extend upon these with versioning information of the identities. Using versioning, we can check to see if the version of the identifiable in either the provenance or the status map is identical to the version of the step. If this is not the case, (or if the current version is a SNAPSHOT version in the future?) we can send the data in as "Modified", and append the version number to it from there. Suddenly, this is more than fault tolerance; this is )


Case Study: Imperative Steps
In some pipelines, we there could potentially be steps where the data is looked over again and again, or where the data that we look at may change as time goes on. From this, we can model the behavior of checking the data over and over again either by modeling that imperative data as a Cyclic pipeline. From there, we can either pass reactive data through that pipeline, or allow repeated data to go through, being more imperative, depending on the connector that's being used. 

At the very start of the Fixr Pipeline, we need to clone and check out commits of Github repos in order to analyze those repos. However, Git Repos themselves are imperative, as new commits can be pushed into the repo at any point. In this sense, we would need to keep checking the Git Repos to see if they are being pushed. We can model this behavior with a pipeline similar to toClone:--Clone()-->cloned:--GetCommits()-->(commits, toPull:--Pull()-->cloned). This pipeline will push clone Github Repos, and then when getting new commits, will also push the repo into the toPull pipeline, which will then allow it to be pulled and sent back to the cloned:--GetCommits()-->commits step. However, while the other steps can be simple streaming steps, doing the same to this step will lead to an infinite computation loop doing absolutely nothing most of the time, wasting computation power. To prevent this, we can change the connector of this step to wait a week before it actually pulls the repos through the configuration file. From there, we would either need to change the commit extraction step to not have a status map, as a status map would most likely block the repos coming back from the Pull step from running if there were no new commits, or take a more reactive approach and add a date state to the repos to allow the pipeline to treat it as different data.

In something like Summingbird, this would be near impossible unless you ended up having at least 3 instances of MapReduce available. In regular MapReduce or Hadoop, this may actually be impossible unless you could 

Case Study: Groums
Groums are a pretty weird case in our Fixr Pipeline. Within this part of the pipeline, groums are created via a pair between an APK and a github repo. Groums then need to be accumulated in a group based on where they came from, which can be done by accumulating groums via part of the provenance information, which we have due to the provenance curators. From there, groums need to be analyzed to form a frequent itemset within those groups, leaving us with a cluster of groums that we need to analyze. In this case, we can use the provenance information embedded into the Identifiables of the groums in order to group them together, and store in the File System as a cluster. From there, we can use mapping to form the frequent item set within the cluster of groums. 

Case Study: Expensive Computation Evasion
Our pipeline may be on OpenStack or AWS in the future, or parts of it could be on those locations. However, the OpenStack or AWS instance that we could use could potentially go down in the middle of our computations. Alternatively, let’s say that a step near the end of the pipeline was proven to not be correct, and thus the pipeline would have to be shut down to fix that specific step. With a normal pipeline, both of these circumstances would be devastating, as you would potentially have to recompute items over and over again, and you might even have to throw away (or at least move) some of your intermediate data. Moreover, if there were an expensive computation halfway through the pipeline, you would be wasting resources recomputing that section over again. With versioning and the idea of persistence of BigGlue, if the pipeline fails for whatever reason, it could just start back up as though nothing had happened, and an outage wouldn’t waste a single bit of computing power.
Let's take the Fixr Pipeline again as an example. Throughout the beginning of the pipeline, we will need to extract commits from Github Repos to then use for our other steps. However, changing these other steps may require extracting commits from the repos again, which is unnecessary. By calling a persist/run method on the pipeline, we can rebuild the status maps and and use the information there to send down the data that was marked as either Not Done, Modified, or as an Error. As such, we can bypass the need to "clone" and extract the commits, saving a bit of computation time.
We can actually measure this empirically in this case. We can run a pipeline through an expensive computation during one section (modeled by a Python script) and then put this through some steps, one of which we may change the version number. We can then time how long it would take to do the pipeline normally. Then, we can change the version number of the step, and see how long it will take to run the program with and without versioned fault tolerance.

Case Study: Verification of Results
In order to actually prove that our pipeline is actually accurate, the developers of a pipeline would want to actually look at the results of a computation and see if those results are as expected. As such, when looking at the database or the datamap of the stored data, if a developer has provenance tracking turned on, they can see the identity of the original data, since the provenance data is appended to the stored Identifiable. They can also potentially check it through a modified Provenance curator; When applying a provenance to an Identifiable, it can then look at that Identifiable in the provenance map to see if it already has a provenance. If so, then the developer would be able to compare the provenances in order to at least verify that the results are stable.

Case Study: Github Repo Service (DO NOT INCLUDE?)
This is a fairly silly case study that has no real effect on creating pipelines, but it shows a kind of cool extension to what you could potentially do with the abstraction of data stores! In this service, we have to be able to clone and get information out of a Github Repo. To do this, we have an abstraction of the FileSystemDataMap, with the subdirectory pointed at the repo folder. The FileSystemDataMap maps different files to the contents of those files, but can also match directories (finished with '/') with the files within those directories.  Whenever an item is cloned, the Git Repo needs to be put in an empty folder with its contents. To do this, we end up creating a folder for the repo, (or two), and then we use "git clone -C folder" to clone the repo into the folder! If the folder doesn't exist, which we can check easily with a get("folder/") and see if we get None, then we will need to create it, by doing put("folder/"). Then, we are able to clone into the folder! Compared to the Java version, which has you create a file with the name of the subdirectory, see if that file exists and if that's a directory, and then see whether or not it's actually empty, this is slightly easier, and for every call removes 4-5 lines of code. This is a very silly example, but it can show how useful the abstractions are with modeling data, even within the file system. It wouldn't be hard to imagine a difficult case that could be modeled fairly easily as a map, list, or queue.

